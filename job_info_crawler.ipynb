{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc08c0f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import urllib3\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from urllib.error import URLError, HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from config.assistant import *\n",
    "from config.site_config import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "request_headers = {\n",
    "    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                   '(KHTML, like Gecko) Chrome/68.0.3440.75 Safari/537.36'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f7721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_group = {}\n",
    "\n",
    "with open(filter_list_company_name_path) as f:\n",
    "    filter_list_company_name = f.read().splitlines()\n",
    "with open(filter_list_company_name_part_path) as f:\n",
    "    filter_list_company_name_part = f.read().splitlines()\n",
    "\n",
    "with open(filter_list_posting_region_path) as f:\n",
    "    filter_list_posting_region = f.read().splitlines()\n",
    "with open(filter_list_posting_name_part_path) as f:\n",
    "    filter_list_posting_name_part = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f630935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dictionary(job_company, company_link, posting_str, posting_link):\n",
    "    global job_group\n",
    "    \n",
    "    # 신규 회사명 인입\n",
    "    if not job_company in job_group:    \n",
    "        job_group[job_company] = []\n",
    "    \n",
    "    for company in job_group[job_company]:\n",
    "        if company['title'] == posting_str:    # 채용공고명 - 중복\n",
    "            break\n",
    "    else:\n",
    "        job_group[job_company].append({'company': job_company, \\\n",
    "                                       'company_link': company_link, \\\n",
    "                                       'title': posting_str, \\\n",
    "                                       'title_link': posting_link, \\\n",
    "                                       'title_idx': len(job_group[job_company]), \\\n",
    "                                       'input_date': datetime.date.today().isoformat(), \\\n",
    "                                       'status': 'wait'\n",
    "                                      })\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7d4e8",
   "metadata": {},
   "source": [
    "### 회사명 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d6187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_company_name(job_company):\n",
    "\n",
    "    # 회사명 - 특정 단어 제거\n",
    "    for remove_word in remove_words:\n",
    "        if remove_word in job_company:\n",
    "            job_company = job_company.replace(remove_word, '')\n",
    "            \n",
    "    # 회사명 - ()괄호 안의 모든 단어 제거\n",
    "    company_re = re.search('\\(.*\\)', job_company)\n",
    "    if company_re:\n",
    "        str_filtering = job_company[company_re.span()[0]:company_re.span()[1]]\n",
    "        job_company = job_company.replace(str_filtering, '')\n",
    "        \n",
    "    # 회사명 - 완전히 일치하는 회사명 리스트 필터링\n",
    "    if job_company in filter_list_company_name:\n",
    "        return 0, \"none\"\n",
    "    \n",
    "    # 회사명 - 특정 단어가 포함되어 있는 회사 필터링\n",
    "    for remove_word in filter_list_company_name_part:\n",
    "        if remove_word in job_company:\n",
    "            return 0, \"none\"\n",
    "\n",
    "    return 1, job_company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80008f12",
   "metadata": {},
   "source": [
    "### 채용공고명 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40a54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_posting_name(posting_str):\n",
    "\n",
    "    # 지역명이 포함되는 경우 탈락\n",
    "    for remove_word in filter_list_posting_region:\n",
    "        if remove_word in posting_str:\n",
    "            return 0, \"none\"\n",
    "\n",
    "    # 특정 단어가 포함되는 경우 탈락\n",
    "    for remove_word in filter_list_posting_name_part:\n",
    "        if remove_word in posting_str:\n",
    "            return 0, \"none\"\n",
    "\n",
    "    return 1, posting_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8dfae",
   "metadata": {},
   "source": [
    "### JSON 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76bd5357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일 불러오기\n",
    "with open(file_path, 'r') as file:\n",
    "    job_group = json.load(file)\n",
    "print(len(job_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e5d0c",
   "metadata": {},
   "source": [
    "### 사람인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee70547b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컴퓨터비전: 1317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m page_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob_tit\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, page_items\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv.content > div:nth-child(\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# ==============================================\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# 회사명 필터링 \u001b[39;00m\n\u001b[0;32m     50\u001b[0m     company_str \u001b[38;5;241m=\u001b[39m elements\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_event data_layer\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\bs4\\element.py:1973\u001b[0m, in \u001b[0;36mTag.select\u001b[1;34m(self, selector, namespaces, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soupsieve \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1970\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot execute CSS selectors because the soupsieve package is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1971\u001b[0m     )\n\u001b[1;32m-> 1973\u001b[0m results \u001b[38;5;241m=\u001b[39m soupsieve\u001b[38;5;241m.\u001b[39mselect(selector, \u001b[38;5;28mself\u001b[39m, namespaces, limit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;66;03m# We do this because it's more consistent and because\u001b[39;00m\n\u001b[0;32m   1976\u001b[0m \u001b[38;5;66;03m# ResultSet.__getattr__ has a helpful error message.\u001b[39;00m\n\u001b[0;32m   1977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28;01mNone\u001b[39;00m, results)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\__init__.py:144\u001b[0m, in \u001b[0;36mselect\u001b[1;34m(select, tag, namespaces, limit, flags, custom, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\n\u001b[0;32m    133\u001b[0m     select: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    134\u001b[0m     tag: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    141\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"Select the specified tags.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1563\u001b[0m, in \u001b[0;36mSoupSieve.select\u001b[1;34m(self, tag, limit)\u001b[0m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m, limit: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1561\u001b[0m     \u001b[38;5;124;03m\"\"\"Select the specified tags.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1568\u001b[0m, in \u001b[0;36mSoupSieve.iselect\u001b[1;34m(self, tag, limit)\u001b[0m\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m, limit: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1566\u001b[0m     \u001b[38;5;124;03m\"\"\"Iterate the specified tags.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1568\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m CSSMatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselectors, tag, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamespaces, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags)\u001b[38;5;241m.\u001b[39mselect(limit):\n\u001b[0;32m   1569\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m el\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1468\u001b[0m, in \u001b[0;36mCSSMatch.select\u001b[1;34m(self, limit)\u001b[0m\n\u001b[0;32m   1465\u001b[0m lim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m limit \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m limit\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_descendants(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtag):\n\u001b[1;32m-> 1468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m child\n\u001b[0;32m   1470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1495\u001b[0m, in \u001b[0;36mCSSMatch.match\u001b[1;34m(self, el)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, el: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbs4.Tag\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;124;03m\"\"\"Match.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_doc(el) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_tag(el) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1414\u001b[0m, in \u001b[0;36mCSSMatch.match_selectors\u001b[1;34m(self, el, selectors)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;66;03m# Verify `nth` matches\u001b[39;00m\n\u001b[1;32m-> 1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_nth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnth\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selector\u001b[38;5;241m.\u001b[39mflags \u001b[38;5;241m&\u001b[39m ct\u001b[38;5;241m.\u001b[39mSEL_EMPTY \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_empty(el):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:907\u001b[0m, in \u001b[0;36mCSSMatch.match_nth\u001b[1;34m(self, el, nth)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nth:\n\u001b[0;32m    906\u001b[0m     matched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mselectors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectors\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(el)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:1399\u001b[0m, in \u001b[0;36mCSSMatch.match_selectors\u001b[1;34m(self, el, selectors)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;66;03m# Verify tag matches\u001b[39;00m\n\u001b[1;32m-> 1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1400\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;66;03m# Verify tag is defined\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:756\u001b[0m, in \u001b[0;36mCSSMatch.match_tag\u001b[1;34m(self, el, tag)\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_namespace(el, tag):\n\u001b[0;32m    755\u001b[0m         match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_tagname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    757\u001b[0m         match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m match\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:745\u001b[0m, in \u001b[0;36mCSSMatch.match_tagname\u001b[1;34m(self, el, tag)\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[38;5;124;03m\"\"\"Match tag name.\"\"\"\u001b[39;00m\n\u001b[0;32m    742\u001b[0m name \u001b[38;5;241m=\u001b[39m (util\u001b[38;5;241m.\u001b[39mlower(tag\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_xml \u001b[38;5;129;01mand\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    744\u001b[0m     name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    746\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackerrank\\lib\\site-packages\\soupsieve\\css_match.py:551\u001b[0m, in \u001b[0;36mCSSMatch.get_tag\u001b[1;34m(self, el)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m\"\"\"Get tag.\"\"\"\u001b[39;00m\n\u001b[0;32m    550\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tag_name(el)\n\u001b[1;32m--> 551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_xml \u001b[38;5;28;01melse\u001b[39;00m name\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic()\n",
    "\n",
    "for search_word in search_words:\n",
    "\n",
    "    item_count = 0\n",
    "    current_page = 1\n",
    "    search_result_num = 0\n",
    "\n",
    "    # =======================================================================================\n",
    "    # 검색 결과 리스트 페이지 별 확인\n",
    "    \n",
    "    while True:\n",
    "        search_link = f'https://www.saramin.co.kr/zf_user/search/recruit'\\\n",
    "            + f'?search_area=main'\\\n",
    "            + f'&search_done=y'\\\n",
    "            + f'&search_optional_item=n'\\\n",
    "            + f'&searchType=search'\\\n",
    "            + f'&recruitSort=relation'\\\n",
    "            + f'&searchword={search_word}'\\\n",
    "            + f'&recruitPage={current_page}'\\\n",
    "            + f'&recruitPageCount={page_view_items}'\\\n",
    "            + f'&company_cd={company_cd}'\\\n",
    "            + f'&mainSearch=y'\n",
    "        response = requests.get(search_link, headers=request_headers, verify=False)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        dom = etree.HTML(str(soup))\n",
    "        \n",
    "        # =======================================================================================\n",
    "        # 종료조건 - 검색 결과가 없으면 종료\n",
    "        \n",
    "        try:\n",
    "            num_str = dom.xpath('//*[@id=\"recruit_info\"]/div[1]/span')[0].text    # 검색단어 리스트 별 검색 결과 개수\n",
    "            search_result_num = int(re.sub(r'[^0-9]', '', num_str))\n",
    "        except:\n",
    "            break\n",
    "        finally:\n",
    "            if current_page == 1:\n",
    "                print(f'{search_word}: {search_result_num}')\n",
    "\n",
    "        # =======================================================================================\n",
    "        # 채용정보 리스트\n",
    "        page_items = len(soup.find_all('h2', attrs={'class': 'job_tit'}))\n",
    "\n",
    "        for i in range(1, page_items+1):\n",
    "            elements = soup.select(f'div.content > div:nth-child({i})')[0]\n",
    "\n",
    "            # ==============================================\n",
    "            # 회사명 필터링 \n",
    "            company_str = elements.find('a', attrs={'class': 'track_event data_layer'}).text\n",
    "            filtering_company_result = filtering_company_name(company_str)\n",
    "            if filtering_company_result[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                company_str = filtering_company_result[1]\n",
    "\n",
    "            # ==============================================\n",
    "            # 채용공고명 필터링\n",
    "            posting_str = elements.find('a', attrs={'class': 'data_layer'})['title']\n",
    "            filtering_posting_result = filtering_posting_name(posting_str)\n",
    "            if filtering_posting_result[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # 채용공고 링크\n",
    "                posting_link = base_link_saram + elements.find('a', attrs={'class': 'data_layer'})['href']\n",
    "\n",
    "                # 회사 링크\n",
    "                company_link = base_link_saram + elements.find('a', attrs={'class': 'track_event data_layer'})['href']\n",
    "\n",
    "            # ==============================================\n",
    "            # Dictionary에 저장\n",
    "            save_dictionary(company_str, company_link, posting_str, posting_link)\n",
    "\n",
    "            item_count += 1\n",
    "        current_page += 1\n",
    "        \n",
    "print(\"--------------------------------------------\")\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50235f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nCnt = 0\n",
    "for i in job_group:\n",
    "    for j in job_group[i]:\n",
    "        nCnt += 1\n",
    "print(nCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a6539",
   "metadata": {},
   "source": [
    "### JSON 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b9454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일 쓰기\n",
    "tf = open(file_path, 'w')\n",
    "json.dump(job_group, tf)\n",
    "tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af43b062",
   "metadata": {},
   "source": [
    "### 잡코리아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff33be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컴퓨터비전: 479\n",
      "COMPUTER VISION: 95\n",
      "영상처리: 675\n",
      "IMAGE PROCESSING: 39\n",
      "딥러닝: 784\n",
      "DEEP LEARNING: 147\n",
      "머신러닝: 1103\n",
      "MACHINE LEARNING: 1104\n",
      "이미지인식: 72\n",
      "IMAGE RECOGNITION: 20\n",
      "이미지분석: 794\n",
      "IMAGE ANALYSIS: 20\n",
      "VISION AI: 213\n",
      "OPENCV: 120\n",
      "로봇비전: 99\n",
      "--------------------------------------------\n",
      "Elapsed time is 538.2712233066559 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 잡코리아\n",
    "# 저장된 키워드 별 검색\n",
    "\n",
    "tic()\n",
    "for search_word in search_words:\n",
    "\n",
    "    item_count = 0\n",
    "    current_page = 1\n",
    "    \n",
    "    # 검색 결과 리스트 페이지 별 확인\n",
    "    search_loop = True\n",
    "    while(search_loop):\n",
    "        search_link = f'https://www.jobkorea.co.kr/Search/' \\\n",
    "                    +f'?stext={search_word}' \\\n",
    "                    +f'&tabType=recruit&Page_No={current_page}'\n",
    "\n",
    "        response = requests.get(search_link, headers=request_headers, verify=False)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # =======================================================================================\n",
    "        # 채용정보 리스트\n",
    "        page_items = len(soup.find_all('a', attrs={'class': 'title dev_view'}))\n",
    "        \n",
    "        if page_items == 0:\n",
    "            search_loop = False\n",
    "        else:\n",
    "            for i in range(0, page_items):\n",
    "\n",
    "                # ==============================================\n",
    "                # 회사명 필터링 \n",
    "                company_str = soup.find_all('a', attrs={'class': 'name dev_view'})[i].text\n",
    "                filtering_company_result = filtering_company_name(company_str)\n",
    "                \n",
    "                if filtering_company_result[0] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    company_str = filtering_company_result[1]\n",
    "                \n",
    "                \n",
    "                # ==============================================\n",
    "                # 채용공고명 필터링\n",
    "                posting_str = soup.find_all('a', attrs={'class': 'title dev_view'})[i].text\n",
    "                posting_str = posting_str.replace('\\r\\n', '')\n",
    "                posting_str = posting_str.strip()\n",
    "                \n",
    "                filtering_posting_result = filtering_posting_name(posting_str)\n",
    "                \n",
    "                if filtering_posting_result[0] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # 채용공고 링크\n",
    "                    posting_link = base_link_korea + soup.find_all('a', attrs={'class': 'title dev_view'})[i]['href']\n",
    "\n",
    "                    # 회사 링크\n",
    "                    company_link = base_link_korea + soup.find_all('a', attrs={'class': 'name dev_view'})[i]['href']\n",
    "                    \n",
    "                # ==============================================\n",
    "                # Dictionary에 저장\n",
    "                save_dictionary(company_str, company_link, posting_str, posting_link)\n",
    "                \n",
    "                item_count += 1\n",
    "        current_page += 1\n",
    "    print(f'{search_word}: {item_count}')\n",
    "    \n",
    "print(\"--------------------------------------------\")\n",
    "toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50559ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2819"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c00a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5373\n"
     ]
    }
   ],
   "source": [
    "nCnt = 0\n",
    "for i in job_group:\n",
    "    for j in job_group[i]:\n",
    "        nCnt += 1\n",
    "print(nCnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abba220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일 쓰기\n",
    "tf = open(file_path, 'w')\n",
    "json.dump(job_group, tf)\n",
    "tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb812a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "019232c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52011d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04967a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a369262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ac6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "061b6b35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # JSON 파일 쓰기\n",
    "# tf = open(file_path, 'w')\n",
    "# json.dump(job_group, tf)\n",
    "# tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996ed0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JSON 파일 불러오기\n",
    "# with open(file_path, 'r') as file:\n",
    "#     job_group = json.load(file)\n",
    "# print(len(job_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b3732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackerrank",
   "language": "python",
   "name": "hackerrank"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
